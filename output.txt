from datetime import datetime
from airflow import DAG
from airflow.providers.google.cloud.operators.bigquery import BigQueryGetDataOperator
from airflow.providers.google.cloud.operators.bigquery_to_gcs import BigQueryToGCSOperator

# Define your DAG parameters
default_args = {
    'owner': 'your_name',
    'start_date': datetime(2024, 4, 12),
    'retries': 1,
}

# Initialize the DAG
dag = DAG(
    'bigquery_to_gcs_dag',
    default_args=default_args,
    schedule_interval=None,  # Set your desired schedule interval
    catchup=False,
)

# Define your BigQuery tables
table1 = 'your_project.your_dataset.table1'
table2 = 'your_project.your_dataset.table2'

# Task 1: Fetch data from Table 1
fetch_table1_data = BigQueryGetDataOperator(
    task_id='fetch_table1_data',
    dataset_id='your_dataset',
    table_id='table1',
    project_id='your_project',
    dag=dag,
)

# Task 2: Fetch data from Table 2
fetch_table2_data = BigQueryGetDataOperator(
    task_id='fetch_table2_data',
    dataset_id='your_dataset',
    table_id='table2',
    project_id='your_project',
    dag=dag,
)

# Task 3: Write data from Table 1 to GCS
write_table1_to_gcs = BigQueryToGCSOperator(
    task_id='write_table1_to_gcs',
    source_project_dataset_table=table1,
    destination_cloud_storage_uris=['gs://your_bucket/table1_output.txt'],
    export_format='CSV',
    field_delimiter=',',
    dag=dag,
)

# Task 4: Write data from Table 2 to GCS
write_table2_to_gcs = BigQueryToGCSOperator(
    task_id='write_table2_to_gcs',
    source_project_dataset_table=table2,
    destination_cloud_storage_uris=['gs://your_bucket/table2_output.txt'],
    export_format='CSV',
    field_delimiter=',',
    dag=dag,
)

# Set task dependencies
fetch_table1_data >> write_table1_to_gcs
fetch_table2_data >> write_table2_to_gcs
