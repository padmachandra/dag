from airflow import DAG
from airflow.providers.google.cloud.operators.bigquery import BigQueryGetDataOperator
from airflow.providers.google.cloud.operators.gcs import GCSToLocalFilesystemOperator
from airflow.providers.google.cloud.operators.gcs import GoogleCloudStorageCreateBucketOperator
from airflow.providers.google.cloud.operators.gcs import GoogleCloudStorageUploadOperator
from datetime import datetime

# Define your project ID, target dataset, and GCS bucket details
project_id = os.environ["GCP_PROJECT"]
target_dataset = "<your-target-dataset>"
gcs_bucket_name = "<your-gcs-bucket-name>"

# List of source tables
table_list = ["table1", "table2"]  # Add more tables as needed

# Create the DAG
dag = DAG(
    "fetch_and_export_bigquery_data",
    schedule_interval="0 0 * * *",  # Daily execution
    start_date=datetime(2024, 2, 27),
    catchup=False,
)

# Fetch data from BigQuery tables
for source_table in table_list:
    fetch_data_op = BigQueryGetDataOperator(
        task_id=f"fetch_{source_table}",
        dataset_id=target_dataset,
        table_id=source_table,
        max_results=1000,  # Adjust as needed
        dag=dag,
    )

    # Generate output file in text format with double quotes
    output_file_path = f"/tmp/{source_table}.txt"
    with open(output_file_path, "w") as output_file:
        for row in fetch_data_op.output:
            output_file.write(f'"{row[0]}", "{row[1]}", ...')  # Customize as per your schema

    # Upload the output file to GCS
    upload_to_gcs_op = GoogleCloudStorageUploadOperator(
        task_id=f"upload_to_gcs_{source_table}",
        bucket_name=gcs_bucket_name,
        object_name=f"{source_table}/{source_table}.txt",
        filename=output_file_path,
        dag=dag,
    )

    # Set up dependencies
    fetch_data_op >> upload_to_gcs_op

# Create the GCS bucket if it doesn't exist
create_bucket_op = GoogleCloudStorageCreateBucketOperator(
    task_id="create_gcs_bucket",
    bucket_name=gcs_bucket_name,
    storage_class="STANDARD",
    location="us-central1",  # Change to your desired location
    dag=dag,
)

# Set up dependencies
create_bucket_op >> fetch_data_op

# Add more tasks or customize as per your requirements
