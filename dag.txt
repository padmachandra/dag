from airflow import models
from google.cloud import bigquery
from google.cloud import storage
from airflow.utils.dates import days_ago

# Define your DAG
with models.DAG(
    "my_bigquery_to_gcs_dag_dynamic",
    schedule_interval=None,  # Set your desired schedule interval
    start_date=days_ago(1),  # Set the start date
    catchup=False,  # Disable backfilling
) as dag:
    # Initialize GCS client
    gcs_client = storage.Client()

    # List of tables to export (add more as needed)
    table_list = ["ABC", "DEF", "GHI"]  # Replace with your actual table names

    for table_name in table_list:
        # Construct BigQuery query
        bq_client = bigquery.Client()
        dataset_id = "your_dataset"  # Replace with your actual dataset ID
        project_id = "your_project"  # Replace with your actual project ID
        table_ref = bq_client.dataset(dataset_id).table(table_name)
        query_job = bq_client.query(f"SELECT * FROM `{table_ref}`")
        rows = query_job.result()

        # Export data to GCS
        bucket_name = "your-gcs-bucket"  # Replace with your actual GCS bucket name
        blob_name = f"output_{table_name.lower()}.txt"
        bucket = gcs_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)

        # Write query results to GCS file
        with blob.open("w") as file:
            for row in rows:
                file.write(",".join(map(str, row.values())) + "\n")

        # You can add more tasks here if needed (e.g., sending notifications, etc.)

# Set task dependencies (if any)
# For example, you can set export_task_abc >> another_task
